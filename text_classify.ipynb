{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvi import ViTokenizer, ViPosTagger # thư viện NLP tiếng Việt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import gensim # thư viện NLP\n",
    "import pickle\n",
    "from sklearn import naive_bayes, metrics, preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><strong> AIM: phân loại -keyword - tóm tắt(kmeans cho tiện) - khai phá thông tin liên quan(association rule)</strong></h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>lấy dữ liệu đã được xử lý</h2>\n",
    "<p>dữ liệu phục vụ cho mô hình là một csr matrix, hay nói cách khác là một tập hợp gồm n phần tử, mỗi phần tử là một vector p chiều</p>\n",
    "<p>mỗi chiều của vector là một từ trong tiếng việt, bao gồm cả từ đơn và từ ghép</p>\n",
    "<p>file .pkl dùng để lưu lại các object(list, tuple ...) trong python</p>\n",
    "<p><strong>count_vect_data lưu lại một ma trận có 40000 cột và số hàng tương ứng với số văn bản, mỗi phần tử là số lần xuất hiện của từ trong văn bản</strong><p>\n",
    "<p><strong>tfidf như cont_vect, nhưng giá trị phần tử là tần suất thay vì số lần xuất hiện</strong></p>\n",
    "<p><strong>tfidf_svd là tfidf nhưng đã giảm số chiều(số cột)</strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_count = pickle.load(open(\"count_vect_data.pkl\", \"rb\"))\n",
    "X_test_count = pickle.load(open(\"count_vect_test.pkl\", \"rb\"))\n",
    "X_data_tfidf_svd = pickle.load(open(\"X_data_tfidf_svd.pkl\", \"rb\"))\n",
    "X_test_tfidf_svd = pickle.load(open(\"X_test_tfidf_svd.pkl\", \"rb\"))\n",
    "y_data = pickle.load(open(\"raw_data/y_data.pkl\", \"rb\"))\n",
    "y_test = pickle.load(open(\"raw_data/y_test.pkl\", \"rb\"))\n",
    "X_data = pickle.load(open(\"raw_data/X_data.pkl\", \"rb\"))\n",
    "X_test = pickle.load(open(\"raw_data/X_test.pkl\", \"rb\"))\n",
    "X_data_tfidf = pickle.load(open(\"X_data_tfidf.pkl\", \"rb\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extraction: Extract relevant features from the text data, such as n-grams, parts of speech, or named entities, that can be used to identify sentiment and opinion.\n",
    "this step was implemented after converting tfidf matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>đánh nhãn cho các lớp, do model hoạt động dựa trên dữ liệu số</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Chinh tri Xa hoi', 'Doi song', 'Khoa hoc', 'Kinh doanh',\n",
       "       'Phap luat', 'Suc khoe', 'The gioi', 'The thao', 'Van hoa',\n",
       "       'Vi tinh'], dtype='<U16')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = preprocessing.LabelEncoder()\n",
    "y_data_n = encoder.fit_transform(y_data)\n",
    "y_test_n = encoder.fit_transform(y_test)\n",
    "\n",
    "encoder.classes_ # kết quả: array(['Chinh tri Xa hoi', 'Doi song', 'Khoa hoc', 'Kinh doanh',\n",
    "                 #                 'Phap luat', 'Suc khoe', 'The gioi', 'The thao', 'Van hoa',\n",
    "                 #                 'Vi tinh'], dtype='<U16')\n",
    "\n",
    "#cell nay bo cx dc, cac thuat toan van chay binh thuong"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>build hàm huấn luyện chung để cho các model để thuận tiện thay đổi model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, X_data, y_data, X_test, y_test, is_neuralnet=False, n_epochs=3):       \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_data, y_data, test_size=0.1, random_state=42)\n",
    "    \n",
    "    if is_neuralnet:\n",
    "        classifier.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=n_epochs, batch_size=512)\n",
    "        \n",
    "        val_predictions = classifier.predict(X_val)\n",
    "        test_predictions = classifier.predict(X_test)\n",
    "        val_predictions = val_predictions.argmax(axis=-1)\n",
    "        test_predictions = test_predictions.argmax(axis=-1)\n",
    "    else:\n",
    "        classifier.fit(X_train, y_train)\n",
    "\n",
    "        train_predictions = classifier.predict(X_train)\n",
    "        val_predictions = classifier.predict(X_val)\n",
    "        test_predictions = classifier.predict(X_test)\n",
    "    #% correct classfied\n",
    "    print('accuracy score mertric:')\n",
    "    print(\"Validation accuracy: \", metrics.accuracy_score(val_predictions, y_val))\n",
    "    print(\"Test accuracy: \", metrics.accuracy_score(test_predictions, y_test))\n",
    "    #The best value is 1 and the worst value is 0.\n",
    "    # print('precision score mertric:')\n",
    "    # print(\"Validation precision: \", metrics.precision_score(val_predictions, y_val))\n",
    "    # print(\"Test precision: \", metrics.precision_score(test_predictions, y_test))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Tiến hành build các model</h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1, Naive-bayes</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score mertric:\n",
      "Validation accuracy:  0.9164490861618799\n",
      "Test accuracy:  0.7852889667250438\n"
     ]
    }
   ],
   "source": [
    "naivBayesType = naive_bayes.MultinomialNB()\n",
    "train_model(naivBayesType, X_data_count, y_data, X_test_count, y_test, is_neuralnet=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2, SVM</h2>\n",
    "<p>svm và knn test dữ liệu có số chiều nhỏ<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score mertric:\n",
      "Validation accuracy:  0.9295039164490861\n",
      "Test accuracy:  0.7957968476357268\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "svmModel = svm.SVC()\n",
    "train_model(svmModel, X_data_tfidf_svd, y_data, X_test_tfidf_svd, y_test, is_neuralnet=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3, KNN</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score mertric:\n",
      "Validation accuracy:  0.8459530026109661\n",
      "Test accuracy:  0.6612959719789843\n"
     ]
    }
   ],
   "source": [
    "from sklearn import neighbors\n",
    "NB_model = neighbors.KNeighborsClassifier()\n",
    "train_model(NB_model, X_data_tfidf_svd, y_data, X_test_tfidf_svd, y_test, is_neuralnet=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>decision tree</h2>\n",
    "<p>min_samples_split:2 if you are splitting a node at minimum it should have 2 records which after splitting into two nodes will give 1 record each which is specified by the min_samples_leaf.:( minimum number of samples required to split an internal node.)</p>\n",
    "<p>split will not happen if there are less than a certain number of records specified by min_samples_split in a node.(minimum number of samples required to be at a leaf node)</p>\n",
    "<strong>these parameters help control the depth, size, and shape of the tree</strong>\n",
    "<p>sample: row-theo matrix</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score mertric:\n",
      "Validation accuracy:  0.6762402088772846\n",
      "Test accuracy:  0.5516637478108581\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree \n",
    "DecisionTree_model = tree.DecisionTreeClassifier(criterion='gini')#dung entropy co ve do chinh xac khong bawng gini\n",
    "train_model(DecisionTree_model, X_data_count, y_data, X_test_count, y_test, is_neuralnet=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>random forest</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score mertric:\n",
      "Validation accuracy:  0.8642297650130548\n",
      "Test accuracy:  0.6854640980735551\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RF_model = RandomForestClassifier(criterion='entropy')\n",
    "train_model(RF_model, X_data_count, y_data, X_test_count, y_test, is_neuralnet=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> sử dụng model </h2>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>tạo mẫu</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "các tư thế xử lý dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(token_pattern=&#x27;\\\\w{1,}&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(token_pattern=&#x27;\\\\w{1,}&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer(token_pattern='\\\\w{1,}')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "#chuyển data về kiểu tfidf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', max_features=30000)\n",
    "tfidf_vect.fit(X_data) # learn vocabulary and idf from training set\n",
    "#chuyển data về kiểu giảm số chiều\n",
    "\n",
    "#nếu không có random state, mỗi lần chạy kết quả sẽ khác nhau\n",
    "svd = TruncatedSVD(n_components=300, random_state=42)\n",
    "svd.fit(X_data_tfidf)\n",
    "#chuyển data về kiểu đếm số lần xuất hiện \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(X_data)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>preprocessing theo kiểu tiếng việt</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mancity đang có phong_độ tốt trước trận tứ_kết lượt đi champions league trên sân_nhà gặp bayern đoàn quân của pep guardiola hiểu rằng họ cần có một kết_quả tốt trước khi làm_khách trên sân đối_thủ lượt về vì_vậy mancity chủ_động dâng cao đội_hình tấn_công gây sức_ép lên đối_phương ngay sau tiếng còi khai_cuộc phút de bruyne chuyền bóng cho haaland từ bên trái cầu_thủ người na uy bứt tốc nhanh và dứt_điểm uy_lực từ bên trái vòng cấm_địa bóng đi vọt xà phút sau haaland khiến cổ_động_viên bayern thót_tim khi lao vào rất nhanh tranh bóng với thủ_thành sommer ngay trên vạch vôi thủ_môn đội khách xử_lý chậm anh phá bóng đập chân haaland nhưng may_mắn trái bóng đã văng ra biên haaland thi_đấu nổi_bật trên hàng công của man city các đồng_đội cũng tập_trung nhiều bóng cho anh phút haaland thoải_mái dứt_điểm đầu vòng cấm_địa tiếc rằng cú sút của anh hơi nhẹ bóng đi chìm và hướng thẳng vào vị_trí của sommer nên không gây nguy_hiểm\n"
     ]
    }
   ],
   "source": [
    "test = [\"ManCity đang có phong độ tốt trước trận tứ kết lượt đi Champions League trên sân nhà gặp Bayern. Đoàn quân của Pep Guardiola hiểu rằng họ cần có một kết quả tốt trước khi làm khách trên sân đối thủ ở lượt về, vì vậy ManCity chủ động dâng cao đội hình tấn công gây sức ép lên đối phương ngay sau tiếng còi khai cuộc. Phút 5, De Bruyne chuyền bóng cho Haaland từ bên trái. Cầu thủ người Na Uy bứt tốc nhanh và dứt điểm uy lực từ bên trái vòng cấm địa, bóng đi vọt xà. 9 phút sau, Haaland khiến cổ động viên Bayern thót tim khi lao vào rất nhanh tranh bóng với thủ thành Sommer ngay trên vạch vôi. Thủ môn đội khách xử lý chậm, anh phá bóng đập chân Haaland nhưng may mắn trái bóng đã văng ra biên. Haaland thi đấu nổi bật trên hàng công của Man City, các đồng đội cũng tập trung nhiều bóng cho anh. Phút 22, Haaland thoải mái dứt điểm ở đầu vòng cấm địa. Tiếc rằng cú sút của anh hơi nhẹ, bóng đi chìm và hướng thẳng vào vị trí của Sommer nên không gây nguy hiểm.\"]\n",
    "lines = gensim.utils.simple_preprocess(test[0])#loại bỏ các phần không liên quan, ex: dấu chấm, dấu phẩy\n",
    "lines = \" \".join(lines)\n",
    "lines = ViTokenizer.tokenize(lines)\n",
    "test[0] = lines\n",
    "print(lines)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>chuyển kiểu dữ liệu tf-idf</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "example_count_type = count_vect.transform(test)\n",
    "\n",
    "example = tfidf_vect.transform(test)\n",
    "\n",
    "example_truncated = svd.transform(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(example_truncated))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dự đoán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kết quả của naive-bayes model: ['The thao']\n",
      "kết quả của SVM model: ['The thao']\n",
      "kết quả của KNN model: ['The thao']\n",
      "kết quả của DT model: ['The thao']\n",
      "kết quả của RF model: ['The thao']\n"
     ]
    }
   ],
   "source": [
    "print(\"kết quả của naive-bayes model: {}\".format(naivBayesType.predict(example_count_type)))\n",
    "print(\"kết quả của SVM model: {}\".format(svmModel.predict(example_truncated)))\n",
    "print(\"kết quả của KNN model: {}\".format(NB_model.predict(example_truncated)))\n",
    "print(\"kết quả của DT model: {}\".format(DecisionTree_model.predict(example_count_type)))\n",
    "print(\"kết quả của RF model: {}\".format(RF_model.predict(example_count_type)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>keyword extraction</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bóng 0.40895724566074687\n",
      "bayern 0.25206173354962336\n",
      "trái 0.20103160734484218\n",
      "cấm_địa 0.1936749761034522\n",
      "dứt_điểm 0.1706720230636951\n"
     ]
    }
   ],
   "source": [
    "feature_names = tfidf_vect.get_feature_names_out()\n",
    "feature_scores = example.toarray()[0]\n",
    "# Sort the features by their scores\n",
    "sorted_indices = feature_scores.argsort()[::-1]\n",
    "# Print the top 5 features and their scores\n",
    "for i in range(5):\n",
    "    print(feature_names[sorted_indices[i]], feature_scores[sorted_indices[i]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> text mining</h2>\n",
    "<strong> cần tối ưu - cần tiền xử lí văn bản </strong>\n",
    "<p> hiện chưa thể làm việc với văn bản lớn </p>\n",
    "<p> field: keyword extraction, sentiment analysis, and recommendation systems.</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keyword extraction: We can treat each document in a corpus as a transaction and each word as an item, and use the Apriori algorithm to identify frequent word patterns that occur across multiple documents. These frequent word patterns can be used as keywords to summarize the key themes or topics in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mlxtend.preprocessing import TransactionEncoder\n",
    "# from mlxtend.frequent_patterns import apriori, association_rules\n",
    "# import pandas as pd\n",
    "# # Define the documents\n",
    "# documents = X_data\n",
    "\n",
    "# # Tokenize the documents\n",
    "# tokenized_documents = [document.lower().split() for document in documents]\n",
    "\n",
    "# # Encode the tokenized documents\n",
    "# te = TransactionEncoder()\n",
    "# te_ary = te.fit(tokenized_documents).transform(tokenized_documents)\n",
    "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# # Find frequent itemsets using Apriori algorithm\n",
    "# frequent_itemsets = apriori(df, min_support=0.4, use_colnames=True)\n",
    "\n",
    "# # Generate association rules\n",
    "# rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.2)\n",
    "\n",
    "# # Print the association rules\n",
    "# #print(rules)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>text summarization</h2>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>example:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"Hồ Chí Minh luôn luôn đặt những vấn đề kinh tế trong mối quan hệ chặt chẽ với những vấn đề chính trị - xã hội: Chúng ta giành được tự do, độc lập rồi mà dân cứ chết đói, chết rét, thì tự do, độc lập cũng không làm gì. Dân chỉ biết rõ giá trị của tự do, của độc lập khi mà dân được ăn no, mặc đủ. Do vậy, ngay sau khi giành chính quyền về tay nhân dân, Người đã kêu gọi nhân dân cả nước tích cực tăng gia sản xuất, quyết tâm diệt giặc dốt và chỉ rõ trách nhiệm Nếu dân đói, Đảng và Chính phủ có lỗi. Với một nước nông nghiệp, Người đưa ra cơ cấu kinh tế nông - công nghiệp; xem nông nghiệp là mặt trận hàng đầu đảm bảo an ninh lương thực để công nghiệp hoá và là hậu phương vững chắc cho sự nghiệp cách mạng. Nền kinh tế xã hội chủ nghĩa phải được tạo lập trên cơ sở chế độ sở hữu công cộng về tư liệu sản xuất. Hồ Chí Minh là người sớm đưa ra chủ trương phát triển cơ cấu kinh tế nhiều thành phần trong thời kỳ quá độ lên chủ nghĩa xã hội ở nước ta. Chủ tịch Hồ Chí Minh rất chú trọng đến công tác nghiên cứu và phổ biến khoa học kỹ thuật phục vụ sản xuất. Người coi trọng vấn đề quản lý, hạch toán kinh tế, cho đó là chìa khoá phát triển kinh tế quốc dân. Người đề xuất chính sách mở cửa và hợp tác với các nước để thu hút ngoại lực và phát huy nội lực. Người cũng đã bước đầu đề cập đến vấn đề khoán trong sản xuất. Hồ Chí Minh đã đưa ra một định nghĩa về văn hoá với nghĩa rộng: Vì lẽ sinh tồn cũng như mục đích của cuộc sống, loài người mới sáng tạo và phát triển ra những ngôn ngữ, chữ viết, đạo đức, pháp luật, khoa học, tôn giáo, văn học, nghệ thuật, những công cụ sinh hoạt hàng ngày về mặc, ăn, ở và các phương tiện sử dụng. Toàn bộ những sáng tạo và phát minh đó tức là văn hoá. Văn hoá là sự tổng hợp của mọi phương thức sinh hoạt cùng với biểu hiện của nó mà loài người đã sản sinh ra nhằm thích ứng nhu cầu đời sống và đòi hỏi của sự sinh tồn. Theo Hồ Chí Minh, văn hoá có nhiệm vụ chủ yếu là bồi dưỡng con người có tư tưởng đúng và tình cảm cao đẹp; nâng cao dân trí, nghĩa là đề cập tới chức năng giáo dục của văn hoá; bồi dư\"]\n",
    "lines = ViTokenizer.tokenize(text[0])\n",
    "text[0] = lines"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1, split sentence</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hồ_Chí_Minh luôn_luôn đặt những vấn_đề kinh_tế trong mối quan_hệ chặt_chẽ với những vấn_đề chính_trị - xã_hội : Chúng_ta giành được tự_do , độc_lập rồi mà dân cứ chết đói , chết rét , thì tự_do , độc_lập cũng không làm gì .\n",
      "Dân chỉ biết rõ giá_trị của tự_do , của độc_lập khi mà dân được ăn no , mặc đủ .\n",
      "Do_vậy , ngay sau khi giành chính_quyền về tay nhân_dân , Người đã kêu_gọi nhân_dân cả nước tích_cực tăng_gia_sản_xuất , quyết_tâm diệt giặc dốt và chỉ rõ trách_nhiệm Nếu dân đói , Đảng và Chính_phủ có lỗi .\n",
      "Với một nước nông_nghiệp , Người đưa ra cơ_cấu kinh_tế nông - công_nghiệp ; xem nông_nghiệp là mặt_trận hàng_đầu đảm_bảo_an_ninh lương_thực để công_nghiệp_hoá và là hậu_phương vững_chắc cho sự_nghiệp cách_mạng .\n",
      "Nền kinh_tế xã_hội chủ_nghĩa phải được tạo_lập trên cơ_sở chế_độ sở_hữu công_cộng về tư_liệu sản_xuất .\n",
      "Hồ_Chí_Minh là người sớm đưa ra chủ_trương phát_triển cơ_cấu kinh_tế nhiều thành_phần trong thời_kỳ quá_độ lên chủ_nghĩa xã_hội ở nước ta .\n",
      "Chủ_tịch Hồ_Chí_Minh rất chú_trọng đến công_tác nghiên_cứu và phổ_biến khoa_học kỹ_thuật phục_vụ sản_xuất .\n",
      "Người coi_trọng vấn_đề quản_lý , hạch_toán kinh_tế , cho đó là chìa_khoá phát_triển kinh_tế quốc dân .\n",
      "Người đề_xuất chính_sách mở_cửa và hợp_tác với các nước để thu_hút ngoại_lực và phát_huy nội_lực .\n",
      "Người cũng đã bước đầu_đề_cập đến vấn_đề khoán trong sản_xuất .\n",
      "Hồ_Chí_Minh đã đưa ra một định_nghĩa về văn_hoá với nghĩa rộng : Vì lẽ sinh_tồn cũng như mục_đích của cuộc_sống , loài_người mới sáng_tạo và phát_triển ra_những ngôn_ngữ , chữ_viết , đạo_đức , pháp_luật , khoa_học , tôn_giáo , văn_học , nghệ_thuật , những công_cụ sinh_hoạt hàng ngày về mặc , ăn , ở và các phương_tiện sử_dụng .\n",
      "Toàn_bộ những sáng_tạo và phát_minh đó tức_là văn_hoá .\n",
      "Văn_hoá là sự tổng_hợp của mọi phương_thức sinh_hoạt cùng với biểu_hiện của nó mà loài_người đã sản_sinh ra nhằm thích_ứng nhu_cầu đời_sống và đòi_hỏi của sự sinh_tồn .\n",
      "Theo Hồ_Chí_Minh , văn_hoá có nhiệm_vụ chủ_yếu là bồi_dưỡng con_người có tư_tưởng đúng và tình_cảm cao_đẹp ; nâng cao dân_trí , nghĩa_là đề_cập tới chức_năng giáo_dục của văn_hoá ; bồi dư\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(text[0])\n",
    "\n",
    "# In ra các câu được tách\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2, change sentence to vector of word</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "sentences_count_type = count_vect.transform(sentences)\n",
    "print(type(sentences_count_type))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3, Build Kmeans model </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_clusters = 3\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "kmeans = kmeans.fit(sentences_count_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(kmeans, open(\"kmeans_object.pkl\", \"wb\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>4, summary</h3>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>điểm lấy ra để đại diện cho các câu là điểm gần centroid nhất</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "import numpy as np \n",
    "\n",
    "avg = []\n",
    "for j in range(n_clusters):\n",
    "    idx = np.where(kmeans.labels_ == j)[0]\n",
    "    #dùng để sắp xếp thứ tự các câu, = trung bình thứ tự xuất hiện\n",
    "    avg.append(np.mean(idx))\n",
    "#chọn câu đại diện là câu gần trung tâm nhất\n",
    "closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, sentences_count_type)\n",
    "ordering = sorted(range(n_clusters), key=lambda k: avg[k])\n",
    "summary = ' '.join([sentences[closest[idx]] for idx in ordering])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hồ_Chí_Minh luôn_luôn đặt những vấn_đề kinh_tế trong mối quan_hệ chặt_chẽ với những vấn_đề chính_trị - xã_hội : Chúng_ta giành được tự_do , độc_lập rồi mà dân cứ chết đói , chết rét , thì tự_do , độc_lập cũng không làm gì . Toàn_bộ những sáng_tạo và phát_minh đó tức_là văn_hoá . Hồ_Chí_Minh đã đưa ra một định_nghĩa về văn_hoá với nghĩa rộng : Vì lẽ sinh_tồn cũng như mục_đích của cuộc_sống , loài_người mới sáng_tạo và phát_triển ra_những ngôn_ngữ , chữ_viết , đạo_đức , pháp_luật , khoa_học , tôn_giáo , văn_học , nghệ_thuật , những công_cụ sinh_hoạt hàng ngày về mặc , ăn , ở và các phương_tiện sử_dụng .\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> LDA algorithm for text's information extraction</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2304)\t1\n",
      "  (0, 2538)\t4\n",
      "  (0, 2852)\t2\n",
      "  (0, 2943)\t2\n",
      "  (0, 3933)\t4\n",
      "  (0, 4747)\t1\n",
      "  (0, 5602)\t3\n",
      "  (0, 5728)\t1\n",
      "  (0, 5811)\t2\n",
      "  (0, 5955)\t1\n",
      "  (0, 6015)\t2\n",
      "  (0, 6023)\t1\n",
      "  (0, 6060)\t1\n",
      "  (0, 6289)\t1\n",
      "  (0, 6639)\t1\n",
      "  (0, 8033)\t2\n",
      "  (0, 8228)\t2\n",
      "  (0, 9564)\t2\n",
      "  (0, 9602)\t2\n",
      "  (0, 9729)\t1\n",
      "  (0, 10144)\t1\n",
      "  (0, 10271)\t2\n",
      "  (0, 10441)\t1\n",
      "  (0, 10969)\t1\n",
      "  (0, 10989)\t1\n",
      "  :\t:\n",
      "  (3822, 16627)\t3\n",
      "  (3822, 16782)\t1\n",
      "  (3822, 18005)\t1\n",
      "  (3822, 19508)\t1\n",
      "  (3822, 19576)\t2\n",
      "  (3822, 20081)\t1\n",
      "  (3822, 21184)\t1\n",
      "  (3822, 21329)\t1\n",
      "  (3822, 22148)\t3\n",
      "  (3822, 26541)\t1\n",
      "  (3822, 26984)\t1\n",
      "  (3822, 27772)\t1\n",
      "  (3822, 28617)\t1\n",
      "  (3822, 29028)\t1\n",
      "  (3822, 30210)\t2\n",
      "  (3822, 31899)\t1\n",
      "  (3822, 31944)\t1\n",
      "  (3822, 32366)\t2\n",
      "  (3822, 32681)\t2\n",
      "  (3822, 32683)\t1\n",
      "  (3822, 32695)\t1\n",
      "  (3822, 34021)\t1\n",
      "  (3822, 34477)\t1\n",
      "  (3822, 34669)\t1\n",
      "  (3822, 35705)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X_data_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "và các của có cho\n",
      "Topic #1:\n",
      "là vé người có một\n",
      "Topic #2:\n",
      "polytechnic tài láng helsinki thấn\n",
      "Topic #3:\n",
      "tàu appf sự_cố chìm cựu_binh\n",
      "Topic #4:\n",
      "và có đã các của\n",
      "Topic #5:\n",
      "phù_điêu hiếu_học tấm chiến_đấu đúc\n",
      "Topic #6:\n",
      "là các của và có\n",
      "Topic #7:\n",
      "ve_chai rượu thú thừa phát\n",
      "Topic #8:\n",
      "núi nhậu cấm nướng đồng_bằng\n",
      "Topic #9:\n",
      "ford giữ mù hắn thủ_đoạn\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "# Create a corpus of text documents\n",
    "\n",
    "\n",
    "# Create a CountVectorizer object to convert the text to a matrix of word counts\n",
    "\n",
    "\n",
    "# Convert the corpus to a matrix of word counts\n",
    "\n",
    "\n",
    "# Apply NMF to the count matrix to factorize it into non-negative components\n",
    "# nmf = NMF(n_components=300)\n",
    "# X_nmf = nmf.fit_transform(X_data_count)\n",
    "\n",
    "# Create an LDA object with 10 topics\n",
    "lda = LatentDirichletAllocation(n_components=10)\n",
    "\n",
    "# Fit the LDA model to the reduced data\n",
    "X_data_count_2 = X_data_count[0:200]\n",
    "lda.fit(X_data_count_2)\n",
    "\n",
    "# Print the top 5 words for each topic\n",
    "feature_names = count_vect.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    print(\" \".join([feature_names[i]\n",
    "                    for i in topic.argsort()[:-6:-1]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
